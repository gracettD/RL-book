{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from typing import Iterable, Iterator, TypeVar, Callable, Mapping, Tuple\n",
    "from rl.distribution import Categorical\n",
    "from rl.iterate import last\n",
    "from rl.markov_decision_process import MarkovDecisionProcess, Policy, TransitionStep, NonTerminal\n",
    "from rl.policy import DeterministicPolicy, RandomPolicy, UniformPolicy\n",
    "import rl.markov_process as mp\n",
    "from rl.markov_process import MarkovRewardProcess\n",
    "from rl.returns import returns\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular MC Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = TypeVar('S')\n",
    "def tabular_mc_prediction(\n",
    "    mrp: MarkovRewardProcess[S],\n",
    "    traces: Iterable[Iterable[Tuple[NonTerminal[S], float]]],\n",
    "    gamma: float,\n",
    ") -> Mapping[NonTerminal[S], float]:\n",
    "    count: Mapping[NonTerminal[S], int] = {s: 0 for s in mrp.non_terminal_states}\n",
    "    sum_return: Mapping[NonTerminal[S], float] = {s: 0.0 for s in mrp.non_terminal_states}\n",
    "    for trace in traces:\n",
    "        for state_return in trace:\n",
    "            state, return_ = state_return\n",
    "            count[state] += 1\n",
    "            sum_return[state] += return_\n",
    "            \n",
    "    value_function: Mapping[NonTerminal[S], float] = {}\n",
    "    for state in mrp.non_terminal_states:\n",
    "        value_function[state] = sum_return[state] / count[state]\n",
    "    return value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on SimpleInventory MRPFinite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.chapter2.simple_inventory_mrp import SimpleInventoryMRPFinite\n",
    "from rl.chapter2.simple_inventory_mrp import InventoryState\n",
    "from rl.distribution import Choose\n",
    "from rl.iterate import last\n",
    "from itertools import islice\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -35.511,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.932,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.345,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.932,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.345,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.345}\n"
     ]
    }
   ],
   "source": [
    "# exact value\n",
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "user_gamma = 0.9\n",
    "si_mrp: SimpleInventoryMRPFinite = SimpleInventoryMRPFinite(\n",
    "    capacity = user_capacity,\n",
    "    poisson_lambda = user_poisson_lambda,\n",
    "    holding_cost = user_holding_cost,\n",
    "    stockout_cost = user_stockout_cost\n",
    ")\n",
    "si_mrp.display_value_function(gamma=user_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -35.51513860043066,\n",
       " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.930186469270527,\n",
       " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.348531656197533,\n",
       " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.93292531437083,\n",
       " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.346984361537285,\n",
       " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.34950462107344}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tabular_MC\n",
    "episodes_num = 60000\n",
    "def episode(\n",
    "    mrp: MarkovRewardProcess[S],\n",
    "    gamma = float,\n",
    "    episode_length_tolerance: float = 1e-6\n",
    "):\n",
    "    max_steps = round(math.log(episode_length_tolerance)/ math.log(gamma))\n",
    "    start_state_distribution = Choose(si_mrp.non_terminal_states)\n",
    "    state: State[S] = start_state_distribution.sample()\n",
    "    reward: float = 0.0\n",
    "    state_reward_pair = []; i = 0;\n",
    "    while isinstance(state, NonTerminal) and i <= 2 * max_steps:\n",
    "        next_distribution = mrp.transition_reward(state)\n",
    "        next_state, reward = next_distribution.sample()\n",
    "        state_reward_pair.append((state,reward))\n",
    "        state = next_state\n",
    "        i = i + 1\n",
    "    \n",
    "    accumulate_return: float = 0.0\n",
    "    state_return_pair = []\n",
    "    for i in range (2*max_steps,-1,-1):\n",
    "        state, reward = state_reward_pair[i]\n",
    "        accumulate_return = gamma * accumulate_return + reward\n",
    "        if i < max_steps:\n",
    "            state_return_pair.append((state, accumulate_return))      \n",
    "    return state_return_pair  \n",
    "\n",
    "episodes = []\n",
    "for i in range (episodes_num):\n",
    "    episodes.append(episode(si_mrp, gamma = user_gamma, episode_length_tolerance = 1e-6))\n",
    "\n",
    "tabular_mc_prediction(si_mrp, episodes, user_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see that the Value Function computed by Tabular Monte-Carlo Prediction with 60000 trace experiences is within 0.01 of the exact Value Function, for each of the states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular TD Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_td_prediction(\n",
    "    mrp: MarkovRewardProcess[S],\n",
    "    traces: Iterable[Iterable[Tuple[NonTerminal[S], float, NonTerminal[S]]]],\n",
    "    gamma: float,\n",
    "    alpha_func: Callable[[int], float]\n",
    ") -> Mapping[NonTerminal[S], float]:\n",
    "    count: Mapping[NonTerminal[S], int] = {s: 0 for s in mrp.non_terminal_states}\n",
    "    value_function: Mapping[NonTerminal[S], float] = {s: 0.0 for s in mrp.non_terminal_states}\n",
    "    for trace in traces:\n",
    "        for state_pair in trace:\n",
    "            state, reward, next_state = state_pair\n",
    "            count[state] += 1\n",
    "            n = count[state]\n",
    "            alpha = alpha_func(n)\n",
    "            value_function[state] = (1-alpha) * value_function[state] + alpha * (reward + gamma * value_function[next_state])\n",
    "    return value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on SimpleInventory MRPFinite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -35.511,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.932,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.345,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.932,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.345,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.345}\n"
     ]
    }
   ],
   "source": [
    "# exact value\n",
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "user_gamma = 0.9\n",
    "si_mrp: SimpleInventoryMRPFinite = SimpleInventoryMRPFinite(\n",
    "    capacity = user_capacity,\n",
    "    poisson_lambda = user_poisson_lambda,\n",
    "    holding_cost = user_holding_cost,\n",
    "    stockout_cost = user_stockout_cost\n",
    ")\n",
    "si_mrp.display_value_function(gamma=user_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -35.5302286312704,\n",
       " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -28.00534413402638,\n",
       " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.328497787464332,\n",
       " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.95427280815482,\n",
       " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.39868260310957,\n",
       " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.3936992936391}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tabular_TD\n",
    "episodes_num = 60000\n",
    "episode_length: int = 100\n",
    "initial_learning_rate: float = 0.03\n",
    "half_life: float = 1000.0\n",
    "exponent: float = 0.5\n",
    "gamma: float = 0.9\n",
    "\n",
    "# learning rate function\n",
    "def learning_rate_schedule(\n",
    "    initial_learning_rate: float,\n",
    "    half_life: float,\n",
    "    exponent: float\n",
    ") -> Callable[[int], float]:\n",
    "    def lr_func(n: int) -> float:\n",
    "        return initial_learning_rate * (1 + (n - 1) / half_life) ** -exponent\n",
    "    return lr_func\n",
    "\n",
    "def episode(\n",
    "    mrp: MarkovRewardProcess[S],\n",
    "    episode_length: int \n",
    "):\n",
    "    start_state_distribution = Choose(si_mrp.non_terminal_states)\n",
    "    state: State[S] = start_state_distribution.sample()\n",
    "    state_reward_pair = []; \n",
    "    for i in range (episode_length):\n",
    "        next_distribution = mrp.transition_reward(state)\n",
    "        next_state, reward = next_distribution.sample()\n",
    "        state_reward_pair.append((state,reward,next_state))\n",
    "        state = next_state  \n",
    "    return state_reward_pair \n",
    "\n",
    "episodes = []\n",
    "for i in range (episodes_num):\n",
    "    episodes.append(episode(si_mrp, episode_length))\n",
    "\n",
    "\n",
    "tabular_td_prediction(si_mrp, episodes, user_gamma, learning_rate_schedule(initial_learning_rate, half_life, exponent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see that the Value Function computed by Tabular Monte-Carlo Prediction with 60000 trace experiences is within 0.06 of the exact Value Function, for each of the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to generate traces using yield? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransitionStep(state=NonTerminal(state=InventoryState(on_hand=1, on_order=0)), next_state=NonTerminal(state=InventoryState(on_hand=0, on_order=1)), reward=-4.678794411714423)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'TransitionStep' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-94bc14c6bdb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'TransitionStep' object is not an iterator"
     ]
    }
   ],
   "source": [
    "trace_gen = si_mrp.simulate_reward(start_state_distribution)\n",
    "trace = next(trace_gen)\n",
    "print(trace)\n",
    "while True:\n",
    "    try:\n",
    "        next(trace)\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
