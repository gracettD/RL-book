{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from rl.distribution import Categorical\n",
    "from typing import Iterable, Iterator, TypeVar, Callable, Mapping, Tuple, Set, Sequence\n",
    "from rl.markov_decision_process import MarkovDecisionProcess, Policy, \\\n",
    "    TransitionStep, NonTerminal, FiniteMarkovDecisionProcess\n",
    "from rl.approximate_dynamic_programming import NTStateDistribution\n",
    "from rl.function_approx import learning_rate_schedule, LinearFunctionApprox, Weights\n",
    "from rl.distribution import Choose\n",
    "import rl.iterate as iterate\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "from rl.function_approx import Tabular\n",
    "from rl.policy import DeterministicPolicy\n",
    "from rl.monte_carlo import greedy_policy_from_qvf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = TypeVar('S')\n",
    "A = TypeVar('A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSPI Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTDQ update\n",
    "def lstdq_update(\n",
    "    transitions: Iterable[TransitionStep[S,A]],\n",
    "    feature_functions: Sequence[Callable[[S,A], float]],\n",
    "    target_policy: DeterministicPolicy[S,A],\n",
    "    gamma: float,\n",
    "    epsilon: float\n",
    ") -> LinearFunctionApprox[Tuple[NonTerminal[S], A]]:\n",
    "    num_feature = len(feature_functions)\n",
    "    A_inv = np.eye(num_feature)/epsilon\n",
    "    b = np.zeros((num_feature,1))\n",
    "    for tr in transitions:\n",
    "        phi1 = np.array([f((tr.state, tr.action)) for f in feature_functions])\n",
    "        if isinstance (tr.next_state, NonTerminal):\n",
    "            phi2 = phi1 - gamma * np.array(\n",
    "                [f((tr.next_state, target_policy.action_for(tr.next_state))) \n",
    "                 for f in feature_functions])\n",
    "        else:\n",
    "            phi2 = phi1\n",
    "        A_inv = A_inv - (A_inv.dot(phi1).dot(phi2.T).dot(A_inv)) / (1 + phi2.T.dot(A_inv).dot(phi1))\n",
    "        b = b + phi1 * tr.reward\n",
    "    w = A_inv.dot(b)\n",
    "    return LinearFunctionApprox.create(feature_functions = feature_functions,\n",
    "                                       weights = Weights.create(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lspi(\n",
    "    transitions: Iterable[TransitionStep[S,A]],\n",
    "    actions: Callable[[NonTerminal[S]], Iterable[A]],\n",
    "    initial_target_policy: DeterministicPolicy[S, A],\n",
    "    feature_functions: Sequence[Callable[[S,A], float]],\n",
    "    gamma: float,\n",
    "    epsilon: float\n",
    ") -> Iterator[LinearFunctionApprox[Tuple[NonTerminal[S], A]]]:\n",
    "    target_policy = initial_target_policy\n",
    "    transition_seq = list(transitions)\n",
    "    while True:\n",
    "        q = lstdq_update(\n",
    "            transitions = transition_seq,\n",
    "            feature_functions = feature_functions,\n",
    "            target_policy = target_policy,\n",
    "            gamma = gamma,\n",
    "            epsilon = epsilon\n",
    "        )\n",
    "        target_policy = greedy_policy_from_qvf(q, actions)\n",
    "        yield q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
