{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from rl.distribution import Categorical\n",
    "from typing import Iterable, Iterator, TypeVar, Callable, Mapping, Tuple, Set\n",
    "from rl.markov_decision_process import MarkovDecisionProcess, Policy, \\\n",
    "    TransitionStep, NonTerminal, FiniteMarkovDecisionProcess\n",
    "from rl.approximate_dynamic_programming import NTStateDistribution\n",
    "from rl.function_approx import learning_rate_schedule\n",
    "from rl.distribution import Choose\n",
    "import rl.iterate as iterate\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "from rl.function_approx import Tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = TypeVar('S')\n",
    "A = TypeVar('A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular SARSA algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action_tabular(\n",
    "    q: Mapping[Tuple[S, A], float],\n",
    "    nt_state: NonTerminal[S],\n",
    "    actions: Set[A],\n",
    "    epsilon: float\n",
    ") -> A:\n",
    "    greedy_action: A = max(((a, q[(nt_state, a)]) for a in actions), key=itemgetter(1))[0]\n",
    "    return Categorical({a: epsilon / len(actions) + \n",
    "                        (1 - epsilon if a == greedy_action else 0.) for a in actions}).sample()\n",
    "\n",
    "def glie_sarsa_tabular(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    states: NTStateDistribution[S],   # initial_state_distribution\n",
    "    gamma: float,\n",
    "    epsilon_as_func_of_episodes: Callable[[int], float],\n",
    "    max_episode_length: int,\n",
    "    count_to_weight_func: Callable[[int], float]\n",
    ") -> Iterator[Mapping[Tuple[S, A], float]]:\n",
    "    q = {(s, a): 0.0 for s in mdp.non_terminal_states for a in mdp.actions(s)}\n",
    "    count = q.copy()\n",
    "    yield q\n",
    "    num_episode: int = 0\n",
    "    \n",
    "    while True:\n",
    "        num_episode += 1\n",
    "        epsilon = epsilon_as_func_of_episodes(num_episode)\n",
    "        state = states.sample()\n",
    "        action = epsilon_greedy_action_tabular(q, nt_state = state, \n",
    "                                               actions = set(mdp.actions(state)),\n",
    "                                              epsilon = epsilon)\n",
    "        steps = 0\n",
    "        while isinstance(state, NonTerminal) and steps < max_episode_length:\n",
    "            next_state, reward = mdp.step(state, action).sample()\n",
    "            if isinstance(state, NonTerminal):\n",
    "                next_action = epsilon_greedy_action_tabular(q, nt_state = next_state, \n",
    "                                                            actions = set(mdp.actions(next_state)),\n",
    "                                                            epsilon = epsilon)\n",
    "                count[(state, action)] += 1\n",
    "                learning_rate = count_to_weight_func(count[(state, action)])\n",
    "                q[(state, action)] = q[(state,action)] + learning_rate * (reward + gamma * q[(next_state, next_action)] - q[(state, action)])\n",
    "                action = next_action\n",
    "            else:\n",
    "                q[(state,action)] = q[(state,action)] + learning_rate * (reward - q[(state, action)])\n",
    "            yield q\n",
    "            steps += 1\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Tabular SARSA to function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.td import glie_sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular_SARSA_control\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.750359886683103,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.68758766212732,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.758661238766457,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.156320557298695,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.451493482298687,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.94751552147776}\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=1)): 1,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): 0,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): 1,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): 0,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): 0,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=0)): 1}\n",
      "Approximation_Function_SARSA_control\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.64793797840082,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.752065704621813,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.682197315614925,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.773192813177165,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.791368601629546,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.93523416756954}\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## test on simple inventory mdp\n",
    "from rl.chapter3.simple_inventory_mdp_cap import SimpleInventoryMDPCap\n",
    "from rl.chapter3.simple_inventory_mdp_cap import InventoryState\n",
    "capacity: int = 2\n",
    "poisson_lambda: float = 1.0\n",
    "holding_cost: float = 1.0\n",
    "stockout_cost: float = 10.0\n",
    "gamma: float = 0.9\n",
    "si_mdp: SimpleInventoryMDPCap = SimpleInventoryMDPCap(\n",
    "    capacity=capacity,\n",
    "    poisson_lambda=poisson_lambda,\n",
    "    holding_cost=holding_cost,\n",
    "    stockout_cost=stockout_cost\n",
    ")\n",
    "    \n",
    "num_episodes = 10000\n",
    "max_episode_length: int = 100\n",
    "epsilon_as_func_of_episodes: Callable[[int], float] = lambda k: k ** -0.5\n",
    "initial_learning_rate: float = 0.1\n",
    "half_life: float = 10000.0\n",
    "exponent: float = 1.0\n",
    "gamma: float = 0.9\n",
    "learning_rate_func: Callable[[int], float] = learning_rate_schedule(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    half_life=half_life,\n",
    "    exponent=exponent\n",
    ")\n",
    "\n",
    "# tabular sarsa\n",
    "qvfs = glie_sarsa_tabular(\n",
    "    mdp = si_mdp,\n",
    "    states = Choose(si_mdp.non_terminal_states),\n",
    "    gamma = gamma,\n",
    "    epsilon_as_func_of_episodes = epsilon_as_func_of_episodes,\n",
    "    max_episode_length = max_episode_length,\n",
    "    count_to_weight_func=learning_rate_func\n",
    ")\n",
    "num_updates = num_episodes * max_episode_length\n",
    "final_qvf = iterate.last(itertools.islice(qvfs, num_updates))\n",
    "optimal_q_value = {s: max(final_qvf[(s,a)] for a in si_mdp.actions(s)) \n",
    "                   for s in si_mdp.non_terminal_states}\n",
    "optimal_policy = {s: max(((a, final_qvf[(s, a)]) for a in si_mdp.actions(s)), key=itemgetter(1))[0] \n",
    "                  for s in si_mdp.non_terminal_states}\n",
    "print(\"Tabular_SARSA_control\")\n",
    "pprint(optimal_q_value)\n",
    "pprint(optimal_policy)\n",
    "\n",
    "\n",
    "# function approximation sarsa\n",
    "from rl.chapter11.control_utils import get_vf_and_policy_from_qvf\n",
    "initial_qvf_dict = {(s, a): 0. for s in si_mdp.non_terminal_states for a in si_mdp.actions(s)}\n",
    "approx_0 = Tabular(\n",
    "    values_map = initial_qvf_dict,\n",
    "    count_to_weight_func = learning_rate_func\n",
    ")\n",
    "qvfs = glie_sarsa(\n",
    "    mdp = si_mdp,\n",
    "    states = Choose(si_mdp.non_terminal_states),\n",
    "    approx_0 = approx_0,\n",
    "    gamma = gamma,\n",
    "    epsilon_as_func_of_episodes = epsilon_as_func_of_episodes,\n",
    "    max_episode_length = max_episode_length\n",
    ")\n",
    "num_updates = num_episodes * max_episode_length\n",
    "final_qvf = iterate.last(itertools.islice(qvfs, num_updates))\n",
    "opt_vf, opt_policy = get_vf_and_policy_from_qvf(mdp=si_mdp, qvf=final_qvf)\n",
    "print(\"Approximation_Function_SARSA_control\")\n",
    "pprint(opt_vf)\n",
    "pprint(opt_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Q-Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_tabular(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    states: NTStateDistribution[S],   # initial_state_distribution\n",
    "    gamma: float,\n",
    "    max_episode_length: int,\n",
    "    count_to_weight_func: Callable[[int], float]\n",
    ") -> Iterator[Mapping[Tuple[S, A], float]]:\n",
    "    q = {(s, a): 0.0 for s in mdp.non_terminal_states for a in mdp.actions(s)}\n",
    "    count = q.copy()\n",
    "    yield q\n",
    "    \n",
    "    while True:\n",
    "        state: NonTerminal[S] = states.sample()\n",
    "        steps = 0\n",
    "        while isinstance(state, NonTerminal) and steps < max_episode_length:\n",
    "            action = epsilon_greedy_action_tabular(q, nt_state = state, \n",
    "                                               actions = set(mdp.actions(state)),\n",
    "                                              epsilon = 1)\n",
    "                \n",
    "            next_state, reward = mdp.step(state, action).sample()\n",
    "            if isinstance(state, NonTerminal):\n",
    "                count[(state, action)] += 1\n",
    "                learning_rate = count_to_weight_func(count[(state, action)])\n",
    "                next_return = max(q[next_state,a] for a in mdp.actions(next_state))\n",
    "                q[(state, action)] = q[(state,action)] + learning_rate * (reward + gamma * next_return - q[(state, action)])\n",
    "            else:\n",
    "                q[(state,action)] = q[(state,action)] + learning_rate * (reward - q[(state, action)])\n",
    "            yield q\n",
    "            steps += 1\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Tabular Q-Learning to function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular_SARSA_control\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.793177374064413,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.20224921577488,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.858441189868383,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.244109908333325,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.110641751326625,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.95680896900531}\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=1)): 1,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): 0,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): 1,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): 0,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): 0,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=0)): 1}\n"
     ]
    }
   ],
   "source": [
    "## test on simple inventory mdp\n",
    "\n",
    "# tabular q-learning\n",
    "qvfs = q_learning_tabular(\n",
    "    mdp = si_mdp,\n",
    "    states = Choose(si_mdp.non_terminal_states),\n",
    "    gamma = gamma,\n",
    "    max_episode_length = max_episode_length,\n",
    "    count_to_weight_func = learning_rate_func\n",
    ")\n",
    "num_updates = num_episodes * max_episode_length\n",
    "final_qvf = iterate.last(itertools.islice(qvfs, num_updates))\n",
    "optimal_q_value = {s: max(final_qvf[(s,a)] for a in si_mdp.actions(s)) \n",
    "                   for s in si_mdp.non_terminal_states}\n",
    "optimal_policy = {s: max(((a, final_qvf[(s, a)]) for a in si_mdp.actions(s)), key=itemgetter(1))[0] \n",
    "                  for s in si_mdp.non_terminal_states}\n",
    "print(\"Tabular_SARSA_control\")\n",
    "pprint(optimal_q_value)\n",
    "pprint(optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.td import q_learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
